{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JennEYoon/llm-ml/blob/main/hugging/Hugging_Face_NLP_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "<b>Colab ( https://colab.research.google.com/ ) – simplest way to get started</b>\n",
        "\n",
        "Colab: File→New Notes in Drive to create a new  notebook\n",
        "\n",
        "Example : In a code block type !ls to list the content of the file which could be sample_data.\n",
        "\n",
        "<ul>Install the Libraries\n",
        "<li>!pip install transformers to install the light version</li>\n",
        "<li>!pip install transformers[sentencepiece] to install the full version ( Contains all libraries need for the course)</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "Test:\n",
        "``` python\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "# Result should see a version >= 4.42.4\n",
        "```"
      ],
      "metadata": {
        "id": "DvIdvGI3SbNk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oCjS9ShWuugJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw7Kif3lh9Nt",
        "outputId": "ef21a138-011b-4f04-fb1f-210d63a297ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "from IPython import display\n",
        "from base64 import b64decode\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxVstFfuh-Lu",
        "outputId": "e72e8978-039d-417e-8345-3661835b5752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.42.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## What is NLP\n",
        "\n",
        "NLP Goal : To understand single words and the context of those words\n",
        "\n",
        "Transformers can also be used in Computer Vision, Images etc..\n",
        "\n",
        "Examples:\n",
        "<ul>\n",
        "  <li>Classifying Whole Sentences</li>\n",
        "  <ul>\n",
        "    <li>Sentiment of a review</li>\n",
        "    <li>Detecting if an email is spam</li>\n",
        "    <li>Is sentence grammatically correct</li>\n",
        "    <li>Are two sentences logically related or not</li>\n",
        "  </ul>\n",
        "  <li>Classifying each word in the sentence</li>\n",
        "  <ul>\n",
        "    <li>Identifying the grammatical components of a sentence (noun, verb, adjective)</li>\n",
        "    <li>Identifying named entities ( person, location, organization )</li>\n",
        "  </ul>\n",
        "  <li>Generating Text Context</li>\n",
        "  <ul>\n",
        "    <li>Comleting a prompt with auto-generated text</li>\n",
        "    <li>Filling in the blanks with masked words</li>\n",
        "  </ul>\n",
        "  <li>Extracting an answer from a text</li>\n",
        "  <ul>\n",
        "    <li>Given a question and a context extracting the answer to the question based on the information provided in the context</li>\n",
        "  </ul>\n",
        "  <li>Generating a new sentence from an input text</li>\n",
        "  <ul>\n",
        "    <li>Translating text into another language</li>\n",
        "    <li>Summarizing text</li>\n",
        "  </ul>\n",
        "</ul>\n",
        "\n",
        "## Locations\n",
        "\n",
        "| URL | Description |\n",
        "|-----|-------------|\n",
        "|[Transformers Github Page](https://github.com/huggingface/transformers) | provides functionality to use and create shared models|\n",
        "|[Model Hub](https://huggingface.co/models)| contains thousands of pretrained models anyone can downloaded and used|"
      ],
      "metadata": {
        "id": "QtAVziZxSHX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tranformers\n",
        "\n",
        "### Pipeline Function\n",
        "\n",
        "1. The most high-level API in the Transformers Library which groups all steps to go from rawtext to usable predictions\n",
        "\n",
        "Major Steps:\n",
        "<ol>\n",
        "  <li>Preporcessing to convert all data into numbers</li>\n",
        "  <li>Model</li>\n",
        "  <li>Post Processing makes the output human readable</li>\n",
        "</ol>\n",
        "\n",
        "``` python\n",
        "# An example pipeline\n",
        "# Based on the first parameter select a pregtrained model that has been fine-tuned for sentiment analysis in English\n",
        "# Create the classifier causes the model to be downloaded and cached\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
        "```\n",
        "\n",
        "### Infrerence API\n",
        "\n",
        "&emsp; Models can be tested directly through your browser using the inference API which is avaiable on the Hugging Face Website.\n",
        "\n",
        "&emsp; Play with the model directl on this page by inputting custom text and watching the model process the input data.\n",
        "\n",
        "### Example of Available Pipelines below\n",
        "These pipelines were programmed for specific tasks, but pipelines can have its behaviors customized\n",
        "\n",
        "<b>I did not run the code yet</b>\n",
        "\n"
      ],
      "metadata": {
        "id": "BGQKurfnXEyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KeTa9IHhSWUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An example pipeline\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ],
      "metadata": {
        "id": "VK9w2MnPaxZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j63hLvBWSPO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An example pipeline with a classifier having two senteneces (batch) -- Returns two labels back\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])"
      ],
      "metadata": {
        "id": "2xPOMrAsbG9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-Shot clasification -- Allows you to clasify text that has not been classified and the developer provides the labels\n",
        "# Don't need to fine tune the model on your data to use it ( useful for real time tasks )\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ],
      "metadata": {
        "id": "50m9fz4kbo7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation\n",
        "from  transformers import pipeline\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ],
      "metadata": {
        "id": "qLgc6Jr2b-R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use with a different model.  Ex use distilgpt2 ( lighter version of gpt2 )\n",
        "# max_length -- length of the generated text\n",
        "# num_return_sequence -- numnber of sentences returned\n",
        "# Text generation involves randomness so its normal if you don't get the same results on other machines.\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\"In this course, we will teach you how to\", max_length=30, num_return_sequence=2)"
      ],
      "metadata": {
        "id": "P17EWYemcW7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating text by guessing the next word in a sentence\n",
        "# top_k=2 : Asking for the two most likely values\n",
        "# <mask> -- called the mask token.  Other mdoels might have a different mask token\n",
        "from transformers import pipeline\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ],
      "metadata": {
        "id": "asS5mpbVdLhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify Each Word in a sentence as a Person, Organization or Location and more...\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"ner\")\n",
        "classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "id": "KkiyOKHEfEVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eamples of group_entieties = true -- Make the pipeline group together the different words linked to the same entity (ex. Hugging Face)\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"ner\", grouped_entities=True)\n",
        "classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "id": "9Rs2HX-efeCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Questioning and Answers -- Proivde the location where the question is answered\n",
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn.\",\n",
        ")"
      ],
      "metadata": {
        "id": "Hs9M9KNzgMXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of transforming one language to another\n",
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ],
      "metadata": {
        "id": "uOyL1rOCgx_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting short summaries of articles\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\"I am learning natural language processing which is a very interesting field\")"
      ],
      "metadata": {
        "id": "Eb_WLKfVgjZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Tranformers Work\n",
        "\n",
        "1. Transformer modesl have been trained as language models on large amounts of raw text using self supervised learning and transfer learning<br>\n",
        "\n",
        "Self Supervised Learning\n",
        "<ul>\n",
        "<li>\n",
        "Based on an neural network and functions by processing unlabeled data and automatically generating the associated labels, without human intervention.  This method works by masking part of the training data and training the model to identify this hidden data. This is done by analyzing the structure and characteristics of the unmasked data. This labeled data is then used for the supervised learning stage.\n",
        "</li>\n",
        "<li>\n",
        "Develops a statistical understanding of the language it has been trained on<br>\n",
        "</li>\n",
        "<li>\n",
        "Usually done with a large amount of data.  Good for when you don't have the computing or monetary resources to build an accruate model.\n",
        "</li>\n",
        "</ul>\n",
        "<img src=\"https://drive.google.com/uc?id=188bbQN1Ntg3nh6zhHBoLrSwo6M2g8vaO\"/>\n",
        "\n",
        "<hr>\n",
        "\n",
        "Transfer Learning\n",
        "<ul>\n",
        "<li>\n",
        "Uses the model from the self supervised learning above or downlaod a model from the ModuleHub ( Hugging Face ).  Also, using a pretrained language model will require less computing and monetary resources\n",
        "</li>\n",
        "<li>\n",
        "Act of initializing a model with another model's weights.  The model's weights from the self sueprvised learning neural network copied to the neural network for the application\n",
        "</li>\n",
        "<li>\n",
        "The training focuses on the specific task that the neural network should be built for.\n",
        "</li>\n",
        "</ul>\n",
        "<img src=\"https://drive.google.com/uc?id=1HYcnRzt2nEmA2b5dlkAeysQ2pk8-2MSK\" />\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "To achieve better performance the model's size should be increased and/or the amount of data to be pretrained on should be increased which can be costly financially and compute resources."
      ],
      "metadata": {
        "id": "q-vQNFNMnWh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tranformer Architecture\n",
        "\n",
        "Model is composed of two blocks\n",
        "<ol>\n",
        "  <li>Encoder -- model is optimized to acquire understanding from its input so it receives the input and build a representation of it features</li>\n",
        "  <li>Decoder -- Optmized for generating outputs so the decoder uses the encoders representation of it features along with a single string from the ( Output probabilites ) to generate a target sequence.\n",
        "</ol>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=126iEZtv0QADavAQVmMLWVRLpAgXEKw0M\" /><br>\n",
        "The \"Output (Shifted Right\") is the current word that was selected from the output probabilities.  With thte current word a new word is selected form the \"output probabilities\"\n",
        "\n",
        "Each of these parts can be used independently.\n",
        "<ol>\n",
        "<li> Encoder Only Models -- Sentence Classification and Named Entitiy Recognition</li>\n",
        "<li>Decoder Only Models -- For generative tasks such as text generation</li>\n",
        "<li>Enocoder Decoder Models -- For task that require input such as translation and summariztion</li>\n",
        "</ol>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1qjXrkixyo2WjkCe1Mk9TI81w5iUYpBkp\"></img>\n",
        "\n",
        "### Attention Layers Introduction\n",
        "\n",
        "A layer that tells the model to pay specific attention to certain words in the sentence when dealing with the representation of each word.\n",
        "\n",
        "A word itself has meaning, but meaning is affected by the context ( other words in the sentence )\n",
        "\n",
        "### Transformer Architecture\n",
        "\n",
        "In the encoder the attention layers can use all the words in a sentence to understand the context.  \n",
        "\n",
        "#### Attention Layers\n",
        "The first type of attention layer resides in the decoder pays attention to all past inputs to the decoder.\n",
        "\n",
        "The second type of attention layer resides in encoder so that languages can bi-directional ( left to right or right to left )\n",
        "\n",
        "Attention Mask -- Prevents the model from paying attention to some special words. Ex. Special Padding to make all the inputs the same length.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1zLZcFbO8JkMD8P2l4YJLfhV9A3Hh8cNQ\" />\n",
        "\n",
        "\n",
        "\n",
        "#### Terms\n",
        "\n",
        "|Terms|Definition|\n",
        "|-----|----------|\n",
        "|Archtitecture | Skeleton of the model ( definiton of each layer and each operation that happens within the model) |\n",
        "|CheckPoints| Weights to be loaded into a given archtitecture |\n",
        "| Model | Umbrella term for Architecture or checkpoint |\n",
        "\n",
        "Bert\n",
        "<ol>\n",
        "  <li>BERT is an archtitecture</li>\n",
        "  <li>bert-base-cased -- a set of weight trained by Google</li>\n",
        "  <li>BERT modle bert-based-cased-model</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sEKOIW3R-QSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Models ( Auto Encoding Models )\n",
        "\n",
        "Use only the encoder of the Transformer Model.\n",
        "\n",
        "At each stage the attention layer can access all the words in the inital sentence and have bi-directional attention\n",
        "\n",
        "Pretraining -- Revolves around corrupting a given sentece ( could be random words) and tasking the model with finding or reconstrcuting the initial sentence\n",
        "\n",
        "Examples:\n",
        "<ol>\n",
        "  <li>Albert</li>\n",
        "  <li>Bert</li>\n",
        "  <li>DistilBERT</li>\n",
        "  <li>Electra</li>\n",
        "  <li>RoBERTa</li>\n",
        "</ol>\n",
        "\n",
        "### How does an Encoder work\n",
        "\n",
        "1. Feature Vector or Feature Tensors one sequence of numbers per input word which contains the word and contextulization information ( holds the meaning of the words within the text).  \n",
        "\n",
        "  The dimension is defined by the architecture of the languagemodel.  \n",
        "\n",
        "  Example to -- Would a vector with the numerial represent of \"to\" and contextual information about the words around it.\n",
        "\n",
        "2. Self Attention Mechanmism -- Contexulization information ( releates to different poisitons  or different words in a single sequence)\n",
        "\n",
        "### When should one use an encoder\n",
        "<ul>\n",
        "  <li>Bi-directional: context from the left and the right</li>\n",
        "  <li>Good at extracting information</li>\n",
        "  <li>Sentiment Analysis, Question Answering, Masked Language Modelling</li>\n",
        "  <li>NLU: Natural Language Understanding -- extracts meaning from text and speech</li>\n",
        "</ul>\n",
        "\n"
      ],
      "metadata": {
        "id": "vihTD-evnz5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoders ( Auto Regressive Models )\n",
        "\n",
        "Use Mask Self Attention -- For a given words the attention layers can only access the words poistioned before or after the word.  ( Bi-directional text ( Is it read from left to right or right to left ).\n",
        "\n",
        "Pretraining usually revolves around predicting the next word in the sentence.\n",
        "\n",
        "Auto-Regressive Models re-use their past outputs as inputs in the following steps.\n",
        "\n",
        "<ol>\n",
        "<li>CTRL</li>\n",
        "<li>GPT</li>\n",
        "<li>GPT-2</li>\n",
        "<li>Transformer XL</li>\n",
        "</ol>\n",
        "\n",
        "### When should a Decoder be used for?\n",
        "<ol>\n",
        "<li>Unidirectional: access to their left or right context</li>\n",
        "<li>Casual Language Modeling -- Generating sequences</li>\n",
        "<li>NLG ( Natural Language Generation )</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q28Dw_UWsc5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence to Sequence Models\n",
        "\n",
        "Uses both the encoder and decoder\n",
        "\n",
        "Encoder -- Get a sentence and decodes into a feature vector.\n",
        "\n",
        "Decoder -- Takes the feature array and the current sequnece word ( previously generated by the decoder or perhaps a null string)  and the output is the next word generated byt the decoder.\n",
        "\n",
        "Both the encoder and decoder do not share weights ( separation of components).\n",
        "The encoder can focus on the sequence (Parsing).  The Decoder can be specialized for a different language or even images.\n",
        "\n",
        "What should a Encoder/Decoder be used for\n",
        "<ol>\n",
        "<li>Translation Language Model (transduction) -- Ex. From English to French</li>\n",
        "<li>Many ot Many Translation</li>\n",
        "<li>Summarization</li>\n",
        "</ol>\n",
        "\n",
        "Examples:\n",
        "<ol>\n",
        "  <li>BART</li>\n",
        "  <li>mBART</li>\n",
        "  <li>MarianMT</li>\n",
        "  <li>T5</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "tFq6xdiDxuHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias and Limitations\n",
        "\n",
        "Pretrained models ( Models built by someone else ) usually is trained on large amounts of data ( both accurate and not accurate). Most models are trained on data from the internet.  Evern Bert sufferes from bias and it was trained <b>only on</b> English Wikipedia and BookCorpus.\n",
        "\n",
        "Be aware of getting a sexist, racist or homphobic content and Fine-Tuning the model on your data will not make the intrinsic bias disappear\n",
        "\n",
        "Human Feedback (Reinforcement) is really helpful with bias\n",
        "\n",
        "GAN -- Look it up.\n",
        "\n",
        "PPO - Proximate Polciy Optimization\n",
        "DPO - Sueprvisor Network -- Training a model is good or bad based on human feedback\n",
        "\n",
        "Rank on inference"
      ],
      "metadata": {
        "id": "pdEk4X9k1x4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dm9yg-3Vxmsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scriblings of a madman done while listening to the discussion.\n",
        "\n",
        "RAG -- Need a human to see how well the LLM is doing.  Different than fine tuning (Reinforcement learning with Human Feedback ).  Last piece of the workflow\n",
        "\n",
        "RAG -- Running an ML Different ( ex. pdf in vector database  ) and it will actually look through all the documents and find the paragraph that relivient.  Not training source\n",
        "\n",
        "Quantizaton to reduce the amount of carbon ( research it )\n",
        "\n",
        "GAN ( resarch it )\n",
        "\n",
        "A tensor has 3 or more dimensions\n",
        "\n",
        "Neural Net -- LSTM ( Long Term Short Memory ) -- limited for large documents with alot of context\n",
        "\n",
        "Pass in the sentence for the encoder only.  The encoder is used during training\n",
        "\n",
        "Textual to Contextual for LLM --\n",
        "\n",
        "NoContext -- Fill in a mask, but in some prelimary material.  Combine multiple sentence through a mask.\n",
        "\n"
      ],
      "metadata": {
        "id": "1EDZygVX4Z6y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3Bt60jN6BAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u0Ipis76spUB"
      }
    }
  ]
}